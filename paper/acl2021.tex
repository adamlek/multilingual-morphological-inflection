%
% File acl2021.tex
%
%% Based on the style files for EMNLP 2020, which were
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2021}
\usepackage[utf8]{inputenc}
%\usepackage[T1]{fontenc}
%\usepackage[encoding]{fontenc}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage[noabbrev,capitalize]{cleveref}
\usepackage{xargs}
\usepackage{graphicx}
%\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
%\newcommandx{\jp}[2][1=]{\todo[linecolor=purple,backgroundcolor=purple!25,bordercolor=purple,#1]{#2}}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\newcommand\jp[1]{\textbf{JP: #1}}

\usepackage{pgfplots}
\usepackage{pgfplotstable}
    \pgfplotsset{
        compat=1.9,
        compat/bar nodes=1.8,
    }
    \pgfplotstableread{
        lang natural hallucinated
        tur	100310 0
        olo	100062 0
        vep	100053 0
        sah	100046 0
        por	100041 0
        pol	100039 0
        ara	100027 0
        tyv	100015 0
        kmr	100003 0
        rus	100002 0
        spa	100001 0
        aym	100000 0
        deu	100000 0
        ces	94169 0
        krl	78673 0
        bul	39011 0
        nld	38827 0
        amh	32254 0
        heb	23204 0
        afb	22165 0
        arz	17683 0
        cni	13948 0
        ckb	11577 0
        ind	11072 0
        evn	5216 10000
        see	3801 10000
        ame	2524 10000
        itl	1246 10000
        syc	1217 10000
        bra	1082 10000
        ail	918 10000
        mag	854 10000
        vro	804 10000
        kod	323 10000
        sjo	290 10000
        gup	214 10000
        ckt	132 10000
        lud	128 10000
    }\testdata

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Training Strategies for Neural Multilingual Morphological Inflection}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
something something something something something something
something something something something something something
something something something something something something
something something something something something something
something something something something something something
something something something something something something
something something something 
\end{abstract}

\section{Introduction}

%Changes in strings typically nakes itself known through two processes,
%morphological inflection and derivation. In morphological derivation a
%words part of speech is changed, while in inflection the grammatical
%functions of the word change.

Morphological inflection is the task of transforming a \emph{lemma} to
its \emph{inflected form} given a set of \emph{grammatical features}
(such as \texttt{tense} or \texttt{person}).  The worlds languages
exhibit a large variety of morphological inflection strategies,
including affixing, circumfixing and reduplication, among
others.
%\jp{citation(s) needed}.
Typically, languages
fall into a spectrum ranging from agglutinative to isolating. In
agglutinative languages grammatical features are encoded with
morphemes attached to a lemma, while in isolating languages each
grammatical feature is repersented as a lemma.

Several systems for inflecting words have been implemented both using
statistical \cite{DBLP:conf/eacl/SmitVGK14, kann2016med} and rule-based
methods \cite{DBLP:phd/basesearch/Hulden09}. In rule-based methods rules for
transformatins of strings are identified and applied accordingly. In
this paper we focus on morphological inflection with neural networks,
specifically, multilingual systems. In a multilingual system, a single
model is developed to process multiple languages, such that we can
give it either a word in, for example, Swedish or Evenk that will be
inflected.

This is a challenging problem for several reasons. For many languages
resources are scarce, a multilingual system must try the balance the
training signals from both high-resource and low-resource languages
such that the model learn something about both. Additionally, while
there are similarities in how inflection is performed on the
word-level different languages employ different strategies to inflect
words. A model must the learn to recognize the different morphological
processes and map them to individual languages.  In this paper, we
investigate how far these two issues can be tackled using primarily
different training strategies, as opposed to focusing on model design
(parameters and the operations applied to them).  Of course, in the
end the optimal system will be a combination of a good model design
and a good training regime. 

However, an advantage of finding good training strategies is that they
do not (generally) require additional parameters in the model. This
allows a model to obtain better representations with fewer parameters,
thus increasing the quality of representations without needing
additional encoding parameters.


%In this paper we explore different training schemas for neural
%networks, in a multilingual model for morphological inflection in 37
%different languages.

%We are particularly interested in how far we can get using a simple
%LSTM sequence-to-sequence model with attention, augmented with
%training procedures informed by human heuristics.

We employ a LSTM encoder-decoder architecture with attention as our
base model and consider the following training strategies:

\begin{itemize}
\item Curriculum learning: We augment the order in which the examples are presented to the model based on the loss.
\item Multi-task learning: We predict the formal operations required to transform a lemma into its inflected form.
\item Language-wise label smooothing: We smooth the loss function to not penalize the model as much when it predicts a character from the correct language.
\item Scheduled sampling: We use a probability distribution to determine whether to use the previous output or the gold as input when decoding.
\end{itemize}

%\section{Task} moved to beginning of intro
%As mentioned previously the task of morphological inflection is to
%predict the \emph{inflectional form} of a lemma (the ``base'' form of
%a word) given a set of grammatical features (such as \texttt{tense} or
%\texttt{person}).
%
%- tag
%- lemma
%- prediction (inflected form)

%[perhaps: Describe edition tasks here as well (as auxiliary tasks)]

\section{Data}

The data released cover $37$ languages of varying typology, genealogy,
grammatical features and morphological processes. The data for the
different languges vary greatly in size, from $138$ examples (Ludic)
to $100310$ (Turkish).  For the low-resourced languages we produce
\textbf{hallucinated data} to train on. We follow
\cite{DBLP:conf/emnlp/AnastasopoulosN19}, and produce a new inflected
form by replacing the parts of an existing inflected form which
overlap with the lemma. 

With respect to the work of \citet{DBLP:conf/emnlp/AnastasopoulosN19},
we make the following changes. We identify all subsequences of length
$3$ or more that overlap in the lemma and inflection. We then randomly
sample one of them $R$ as the replacement sequence. 
For each language we compile a set $\mathcal{C}$ containing all
(1,2,3,4)-grams in the language. We construct a string $G$ to replace
$R$ with by sampling from $\mathcal{C}$ until we have a sequence whose
length satisfy: $|R|-2 \leq |(g_0, ..., g_n)| \leq |R|+2$.

%To generate a new
%subsequence we randomly sample a set $G$ of 

%we randomly sample $n$ (1,2,3,4)-grams in the language
%such that $|r|-2 \leq |(g_0, ..., g_n)| \leq |r|+2$ and replace $r$
%with the concatenation of $(g_0, ..., g_n)$. % fix notation...

Additionally, we don't consider subsequences which include a
phonological symbol \footnote{Thus in \cref{fig:hall} a subsequence of
length 2 is selected as replacement, since the larger subsequences
includes the phonological symbol :}.  A schematic of the hallucination
process is shown in \cref{fig:hall}.


\begin{figure}[h]
\centering
\includegraphics[scale=0.5]{hall.pdf}
\caption{\label{fig:hall} A example of the data hallucination process. The sequence $R=\text{ki}$ is replace by $G=\text{tk}$.}
\end{figure}


For each language with fewer
than 10 000 examples, we sample 10 000 new examples. A visualization
of the number of examples in each language is shown in
\cref{fig:data}.

\begin{figure}[ht]
\begin{tikzpicture}
    \begin{axis}[
        ybar stacked,
        ymin=0,
        ymax=105000,
        scaled y ticks={false},
        xtick=data,
        legend style={
            cells={anchor=west},
            legend pos=north east,
        },
        width=\columnwidth*1.15,
        height=150pt,
        /pgf/bar width=4pt,
        reverse legend=true,
        xticklabels from table={\testdata}{lang},
        xticklabel style={text width=1cm,align=center,font=\tiny,rotate=90},
        yticklabel style={text width=1cm,align=center,font=\tiny,rotate=90}
    ]
        \addplot [fill=green!50]
            table [y=natural, meta=lang, x expr=\coordindex]
                {\testdata};
                    \addlegendentry{natural}
        \addplot [fill=blue!50]
            table [y=hallucinated, meta=lang, x expr=\coordindex]
                {\testdata};
                    \addlegendentry{hallucinated}
    \end{axis}
\end{tikzpicture}
\caption{\label{fig:data} Number of natural and hallucinated examples in each language.}
\end{figure}


%We improve upon [REF] in the following way.
%Noting that certain languages in the data use phonological symbols
%and separators, we hallucinate in such a way that we don't break apart
%morphemes.


\section{Method}

In this section the multilingual model and training strategies used
are presented. We employ a single model with shared parmeters
across all languages. 

\subsection{Model}

We employ a encoder-decoder architecture with attention. The first
layer in the model is comprised of an LSTM, which produces a
contextual representation for each character in the lemma.

We encode the tags using a self-attention module (equivalent to a
1-head transformer layer).  This layer does not use any positional
data: indeed the order of the tags does not matter.

We use an LSTM decoder with two attention modules. One attending to
the lemma and one to the tags. For the lemma attention we use a
content-based attention module \cite{graves2014neural,
karunaratne2021robust} which uses cosine similarity.  However, we
found that using content-based attention only causes attention to be
too focused on a single character, and mostly ignores contextual cues
relevant for the generation.

To remedy this, we combine the content-based attention with additive
%attention as follows, where superscript $cos$ indicate cosine
%attention, $add$ additive attention, $h_t$ the hidden state from the
%encoder, $h_d$ the previous time-step from the decoder and $k$ the
%key:
%\jp{
%  1. Double-check the code.
%  2. Use another naming than additive (I think it's multiplicative)}
attention as follows, where superscript $cb$ indicate content-based attention,
$add$ additive attention and $k$ the key:

\begin{align*}
	a^{add} & = \mathsf{softmax}(w^\top\mathsf{tanh}(\mathsf{W}_ak + \mathsf{W}_bh))\\
	att^{add} & = \sum_{t=1}^{T}a_t^{add}h_t^{add}\\
	a^{cb} & = \mathsf{softabs}(cos(k,h))\\
	att^{cb} & = \sum_{t=1}^{T}a_t^{cb}h_t^{cb}\\
	att & = \mathsf{W}[att^{cb}; att^{add}]
\end{align*}
%(in the above $h_t$ refers to the encoder's contextual representation for each character.)
We employ additive attention for the tags. In each step we pass the
concatenation of the character embedding obtained from the previous
step, the lemma attention and the tag attention to the decoder.

% \[\text{input}_t = [e_{t-1}; att_{char}; att_{tag}]\]

\subsection{Multi-task learning}

Instead of predicting the inflected form, one can also predict the
levenshtein operations needed to transform the lemma into the
inflected form; as inspired by \cite{DBLP:conf/conll/MakarovRC17}.
We find that making \emph{both} predictions, as a multitask setup,
improves the performance of the system.

The multi-task setup operates on the character level, thus for each
contextual representation of a character we want to predict an
operation. Because \texttt{add} and \texttt{del} change the length, we
predict two sets of operations, the \textbf{lemma-reductions} and the
\textbf{lemma-additions}. To illustrate, the levenshtein operations
for the word pair (\emph{valatas}, \emph{ei valate}) in Veps (uralic
language related to Finnish) is shown in \cref{fig:ops}.

%We break down the task of applying levenshtein-distance operations
%into two sub-tasks: \textbf{lemma-reductions}, where we predict the copy and
%deletion operations and \textbf{lemma-additions}, where we predict the copy and
%addition operations. 

\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{ops.pdf}
\caption{\label{fig:ops} Levenshtein operations mapped to characters in the lemma and
inflection.}
\end{figure}

In our setup, the task of lemma-reductions in performed by predicting
the \texttt{cp}, \texttt{del}, and \texttt{sub} operations based on
the encoded hidden states in the lemma.  The task of lemma-additions
then is performed by predicting the \texttt{cp}, \texttt{add}, and
\texttt{sub} operations on the characters generated by the decoder.
For both lemma-reductions and lemma-additions we use a classifier with
shared parameters. \footnote{In the future, we'd like to experiment
with including the representations of tags in the input to the classifier.}


%\jp{I don't see how this solves the length problem?}
%To predict the operations, we pass the lemma representations from the
%encoder and the generated inflection representations to a single operation
%classification layer.

%For each sub-task we predict the operation based on the hidden states
%generated by our neural network. In the case of lemma-reductions we
%predict the operation on the hidden-states of the encoded lemma. For
%lemma-additions, we predict the operations on the generated characters
%from the decoder.

\subsection{Curriculum Learning}

We employ a simple curriculum learning strategy where we 
%We use the easy-first curriculum strategy\jp{[REF]} to 
sort the data after each epoch based on the loss of each example. For
all examples in the batch we sort them according to the loss obtained
in the previous epoch, in ascending order such that the easy (low
loss) occurs before the difficult examples (high loss).

For the first epoch, when we dont have any loss refer to for sorting,
we sort the dataset according to the ratio of copy operations to other
operations. We found that this strategy performed better than any of
the other strategies which we tested: fewest addition-operations,
least-grammatical-features, and random.  This strategy causes a small,
but consistent improvement in the loss of the first epoch, which
persists throughout the learning process.\jp{Curriculum learning in
general or the specific thing for the 1st epoch?}

\subsection{Scheduled Sampling}

Commonly, when training an encoder-decoder RNN model, the input at
time-step $t$, is not the output from the decoder at $t-1$, but rather
the gold data.  It has been shown that models trained with this
strategy may suffer at inference time. Indeed, they have never been
exposed to a partially incorrect input in the training phase.  To
address this issue we use scheduled sampling
\cite{DBLP:conf/nips/BengioVJS15}.

We implement a simple schedule for calculating the probability of
using the gold characters or the model's prediction by using a global
sample-probability variable which is updated at each epoch. We start
with a probability \(\rho\) of 100\% to take the gold. At each epoch
we decrease \(\rho\) by 4\%. For each character, we take a sample from
the Bernoulli distribution of parameter \(\rho\) to determine the
decision to make.

\subsection{Training}

We use cross-entropy loss for the character generation loss and 
%We use KL-divergence loss for the characters
%\jp{But the entropy of the
%  label distribution is constant? So this is a red flag. If it's a
%  workaround for a bug in pytorch, this should come down to a
%  footnote.} 
%and cross-entropy loss 
for the operation predictions tasks. Our final loss function consists
of the character generation loss, the lemma-reduction and the
lemma-addition losses summed.

\paragraph{Language-wise Label smoothing} We use language-wise label
smoothing to calculate the loss. This means that we remove $\alpha$
from the probability of the correct character and distribute the same
$\alpha$ uniformly across the probabilities of the characters
belonging to the language of the word. A difficulty is that each
language potentially uses a different set of characters. We calculate
this set using the the training set only--- so it is important to make
$\alpha$ not too large, so that there is not a too big difference
between characters seen in the training set and those not seen.
Indeed, if there were, the model might completely exclude unseen
characters from its prediction at test-time. (We found that
\(\alpha=2.5\%\) is a good value.)

\paragraph{Learning rate decay with a Curriculum} Recall that the training
examples will be sorted by the difficulty in the previous epoch.  We
note that employing a decaying learning rate has the effect that a
model update its parameters \emph{more} on the easy examples and
less on difficult examples. The idea is that the morphological
processes involved in more difficult words can be discovered from the
operations involved in the easier examples.

The hyperparameters used for training are presented in \cref{tab:hp} below.
\begin{table}[h]	
\centering
\begin{tabular}{lc}
\textsc{Hyperparameter} & \textsc{Value} \\
  \hline
  Batch Size & 256 \\
  Embedding dim & 128 \\
  Hidden dim & 256 \\
  Epochs & 30 \\
  Initial LR & 0.001 \\
  Min LR & 0.0000001 \\
  Smoothing-$\alpha$ & 2.5\% \\
\end{tabular} 
\caption{Hyperparameters.}
\label{tab:hp}
\end{table}

\section{Results}

--- use a figure to fit
\begin{table}[h]	
\centering
\begin{tabular}{lc}
\textsc{Language} & \textsc{Accuracy} \\
  \hline
  
\end{tabular} 
\caption{Acc}
\label{tab:accuracy}
\end{table}



\subsection{Ablation Study}

We perform a leave-one-out ablation study to estimate the effect of
our training strategies. The we use the same hyperparameters as
reported in TABLE, but train for 15 epochs.

%\jp{Not sure if ablation is the right word here.}

--- wrong training parameters:
\begin{table}[h]	
\centering
\begin{tabular}{lr}
\textsc{Module} & \textsc{avg. EM}\\
  \hline
  All  & $76.4$ \\
  w/o Curriculum learning & $75.5$\\
  w/o Label smoothing & $72.7$\\
  w/o Multitask & $74.6$ \\
  w/o Scheduled sampling & $75.5$ \\
  
  
\end{tabular} 
\caption{We evaluate the ablation experiment by calulating the exact
match accuracy for each leanguage, then taking the mean over all
lnaguages.}
\label{tab:abl}
\end{table}

\section{Discussion}




\section{Conclusions}


%\section*{Acknowledgments}

%The acknowledgments should go immediately before the references. Do not number the acknowledgments section.
%\textbf{Do not include this section when submitting your paper for review.}
 
\bibliographystyle{acl_natbib}
\bibliography{acl2021}

%\appendix



\end{document}
