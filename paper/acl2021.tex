%
% File acl2021.tex
%
%% Based on the style files for EMNLP 2020, which were
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2021}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage[noabbrev,capitalize]{cleveref}
\usepackage{xargs}
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
\newcommandx{\jp}[2][1=]{\todo[linecolor=purple,backgroundcolor=purple!25,bordercolor=purple,#1]{#2}}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Training Strategies for Neural Multilingual Morphological Inflection}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
something something something something something something
something something something something something something
something something something something something something
something something something something something something
something something something something something something
something something something something something something
something something something 
\end{abstract}

\section{Introduction}

Morphological inflection is the task of transforming a lemma to its
inflected form given a set of grammatical features. 
[Transition to task...]
The task requires a model to recognize which morphological processes
should be applied to the word, such as affixing, deletion or
reduplication, 
[Typological diversity ...]
[Research problems ...]

In this paper we explore different training schemas for neural
networks, in a multilingual model for morphological inflection in 37
different languages.

We are particularly interested in how far we can get using a simple
LSTM sequence-to-sequence model with attention, augmented with
training procedures informed by human heuristics.

In particular, we consider the following training strategies:

\begin{itemize}
\item Curriculum learning: In this module we augment the order in which the examples are presented to the model
\item Multi-task learning: We consider the formal operations required to transform a lemma into its inflected form
\item Language-wise label smooothing: We smooth the loss function when it predict a character from the correct language
\item Scheduled sampling: Rather than decoding with the gold as input, we use a probability distribution to determine whether to use the previous output of the gold as input
\end{itemize}

\section{Related work}

\section{Task}

The task is to ... 

- tag
- lemma
- prediction (inflected form)

[perhaps: Describe edition tasks here as well (as auxiliary tasks)]

\section{Data}

The data released cover 37 languages of varying typology and
geneology, [stuff...]. The data for the different languges vary greatly
in size, presenting a unique challange to multilingual systems.

For the low-resourced languages we produce \textbf{hallucinated data}
to train on. We follow [REF], and produce an inflected from by textual
replacement of the parts of a inflected form which overlap with the
lemma a new lemma. (For example if the training data contains the pair
(\textbf{run},\textbf{run}ning) we may add the pair
(\textbf{cut},\textbf{cut}ning).

\jp{How much of the data is generated this way?}

We improve upon [REF] in the following way.
Noting that certain languages in the data use phonological symbols
and separators, we hallucinate in such a way that we don't break apart
morphemes.\jp{An example will help.}


\section{Method}

In this section the multilingual model and training strategies used
are presented. We employ a single model with shared parmeters
across all languages. 

\subsection{Model}

We employ a encoder-decoder architecture with attention. The first
layer in the model is comprised of an LSTM, which produces a
contextual representation for each character in the lemma.

We encode the tags using a self-attention module (equivalent to a 1-head transformer layer).
This layer does not use any positional data: indeed the order of the tags does not matter.

We use an LSTM decoder with two attention modules. One attending to
the lemma and one to the tags. For the lemma attention we use a
content-based attention module which uses cosine similarity (called
Cosim by [REF]), which facilitates the copy mechanism.  However, we
found that using Cosim attention only causes attention to be too
focused on a single character, and mostly ignored contextual cues
relevant for the generation.

To remedy this, we combine the cosine attention with additive
attention as follows, where superscript $cos$ indicate cosine attention,
$add$ additive attention and $k$ the key:
\jp{
  1. Double-check the code.
  2. Use another naming than additive (I think it's multiplicative)}

\begin{align*}
	a^{add} & = \mathsf{softmax}(k^\top\mathsf{tanh}(W_ah + W_bh))\\
	att^{add} & = \sum_{t=1}^{T}a_t^{add}h_t^{add}\\
	a^{cos} & = \mathsf{softabs}(cos(k,h))\\
	att^{cos} & = \sum_{t=1}^{T}a_t^{cos}h_t^{cos}\\
	att & = W[att^{cos}; att^{add}]
\end{align*}
(in the above $h_t$ refers to the encoder's contextual representation for each character.)
We employ additive attention for the tags. In each step we pass the
concatenation of the character embedding obtained from the previous
step, the lemma attention and the tag attention to the decoder.

% \[\text{input}_t = [e_{t-1}; att_{char}; att_{tag}]\]

\subsection{Multi-task learning}

Instead of predicting the inflected form, one can also predict the
operations needed to transform the lemma into the inflected form
[REF].

We find that making \emph{both} predictions, as a multitask setup,
improves the performance of the system.

\jp{add an example; re-check this later --- mention that changing the length is an issue}
We break down the task of applying levenshtein-distance operations
into two sub-tasks: \textbf{lemma-reductions}, where we predict the copy and
deletion operations and \textbf{lemma-additions}, where we predict the copy and
addition operations. 

For each sub-task we predict the operation based on the hidden states
generated by our neural network. In the case of lemma-reductions we
predict the operation on the hidden-states of the encoded lemma. For
lemma-additions, we predict the operations on the generated characters
from the decoder.

\subsection{Curriculum Learning}

We use the easy-first curriculum strategy [REF] to sort the data after
each epoch. For all examples in the batch we sort them according to
the loss obtained in the previous epoch, in ascending order such that the easy (low loss) occurs
before the difficult examples (high loss).  

For the first epoch, when we dont have any loss for the examples, we
sort the dataset according to the ratio of copy levenshtein-operations
to other operations (delete or add). We found that this strategy
performed better than any of the other strategies which we tested:
fewest addition-operations, least-grammatical-features, and random.

This strategy causes a small, but consistent improvement in the loss
of the first epoch, which persists throughout the learning process.=

\subsection{Scheduled Sampling}

\jp{add a sentence explaining teacher-forcing}

It has been shown that models trained with teacher-forcing may suffer
at inference time. Indeed, they have never been exposed to a partially
incorrect input in the training phase.  To address this issue we use
scheduled sampling [REF].

We implement a simple schedule for calculating the probability of
using the gold characters or the model's prediction by using a global
sample-probability variable which is updated at each epoch. We start
with a probability \(\rho\) of 100\% to take the gold. At each epoch
we decrease it by 4\%. For each character, we take sample from the
Bernoulli distribution of parameter \(\rho\) to determine the
decision to make.

\subsection{Training}

We use KL-divergence loss for the characters\jp{But the entropy of the
  label distribution is constant? So this is a red flag. If it's a
  workaround for a bug in pytorch, this should come down to a
  footnote.} and cross-entropy loss for both the lemma-reduction and
lemma-addition tasks. Our final loss function consists of the
character generation loss, the lemma-reduction and the lemma-addition
losses summed.

\paragraph{Language-wise Label smoothing} We use language-wise label
smoothing to calculate the loss. This means that we remove $\alpha$
from the probability of the correct character and distribute the same
$\alpha$ uniformly across the probabilities of the characters
belonging to the language of the word. However, each language
potentially uses a different set of characters. We calculate this set
using the the training set only--- so it is important to make $\alpha$
not too large, so that the model does not completely exclude unseen
characters from its prediction at test-time. (We found that
\(\alpha=2.5\%\) is a good value.)

\paragraph{Learning rate decay with a Curriculum} The training
examples will be sorted by the difficulty in the previous epoch.  We
note that employing a decaying learning rate has the effect that a
model update its parameters \textit{more} on the easy examples and
less on difficult examples. The idea is that the morphological
processes involved in more difficult words can be discovered from the
operations involved in the easier examples.

The hyperparameters used for training are presented in \cref{tab:hp} below.
\begin{table}[h]	
\centering
\begin{tabular}{lc}
\textsc{Hyperparameter} & \textsc{Value} \\
  \hline
  Batch Size & 256 \\
  Embedding dim & 128 \\
  Hidden dim & 256 \\
  Initial LR & 0.001 \\
  Min LR & 0.0000001 \\
  Smoothing-$\alpha$ & 2.5\% \\
\end{tabular} 
\caption{Hyperparameters.}
\label{tab:hp}
\end{table}

\section{Results}

\begin{table}[h]	
\centering
\begin{tabular}{lc}
\textsc{Language} & \textsc{Accuracy} \\
  \hline
  
\end{tabular} 
\caption{Acc}
\label{tab:accuracy}
\end{table}



\subsection{Ablation Study}

We perform an ablation study to estimate the effect of our various
additional training strategies.\jp{Not sure if ablation is the right word here.}



\begin{table}[h]	
\centering
\begin{tabular}{lc}
\textsc{Module} & \textsc{Accuracy} \\
  \hline
  All  & \\
  Lemma-reduction & \\
  Lemma-addition & \\
  Scheduled sampling & \\
  Label smoothing & \\
  Curriculum learning & \\
\end{tabular} 
\caption{Abl}
\label{tab:abl}
\end{table}

\section{Discussion}

\section{Conclusions}


%\section*{Acknowledgments}

%The acknowledgments should go immediately before the references. Do not number the acknowledgments section.
%\textbf{Do not include this section when submitting your paper for review.}

\bibliographystyle{acl_natbib}
\bibliography{acl2021}

%\appendix



\end{document}
